{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Data Extraction"
      ],
      "metadata": {
        "id": "v9BpqLIfWNMB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The main purpose of this module is to retrieve, transform, clean, and load data from medical CSV files, which will serve as the initial dataset for our healthcare support system."
      ],
      "metadata": {
        "id": "Wv8D1Kf4maEt"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M58jU1sZ-i-v",
        "outputId": "4cb93a28-d8cb-4add-d9a2-0555293692e5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "# Json file Path, saved on google drive of the collaboratos\n",
        "json_path = '/content/drive/MyDrive/Colab Notebooks/Big Data/Final_Project/secret.json'\n",
        "\n",
        "# Loading the json file\n",
        "with open(json_path) as f:\n",
        "  secrets = json.load(f)\n",
        "\n",
        "# Secret info from json\n",
        "#mongo_uri = secrets[\"MONGO_BASE_URI\"]\n",
        "mongo_uri = secrets[\"MONGO_M10_URI\"]\n",
        "collection_string_list = secrets[\"COLLECTION_STRING_LIST\"]"
      ],
      "metadata": {
        "id": "9_54tMB5AZRJ"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "dataset_path = '/content/drive/MyDrive/Colab Notebooks/Big Data/Final_Project/Dataset/'"
      ],
      "metadata": {
        "id": "ucT9fwYpC44s"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Retrieve\n"
      ],
      "metadata": {
        "id": "7qJsCu2IlXtL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "For the first thing we have to retrieve from csv files all data as our database. We will clean the data, but to do this we need a framework that scales on horizontal cluster. Let's use spark!"
      ],
      "metadata": {
        "id": "gklBHOPHldzX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Install Java8\n",
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "\n",
        "# download spark3.4.4 (list of mirrors)\n",
        "#!wget -q https://apache.osuosl.org/spark/spark-3.4.4/spark-3.4.4-bin-hadoop3.tgz\n",
        "#!wget -q https://dlcdn.apache.org/spark/spark-3.4.4/spark-3.4.4-bin-hadoop3.tgz\n",
        "!wget -q https://archive.apache.org/dist/spark/spark-3.4.4/spark-3.4.4-bin-hadoop3.tgz\n",
        "\n",
        "# unzip it\n",
        "!tar xf spark-3.4.4-bin-hadoop3.tgz\n",
        "\n",
        "# install findspark\n",
        "!pip install -q findspark\n",
        "\n",
        "# Scarica il connettore MongoDB-Spark\n",
        "!wget -q https://repo1.maven.org/maven2/org/mongodb/spark/mongo-spark-connector_2.12/10.4.1/mongo-spark-connector_2.12-10.4.1.jar\n",
        "!wget -q https://repo1.maven.org/maven2/org/mongodb/mongodb-driver-sync/4.10.2/mongodb-driver-sync-4.10.2.jar\n",
        "!wget -q https://repo1.maven.org/maven2/org/mongodb/bson/4.10.2/bson-4.10.2.jar\n",
        "!wget -q https://repo1.maven.org/maven2/org/mongodb/mongodb-driver-core/4.10.2/mongodb-driver-core-4.10.2.jar\n",
        "\n"
      ],
      "metadata": {
        "id": "ul-pIWVCohYx"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the second part of this notebook, we will load all the cleaned data to a MongoDB server using a MongoDB Atlas connection URI. To do this directly with PySpark, we need to use a dedicated connector. The following cells will contain its configuration."
      ],
      "metadata": {
        "id": "bDq87POpycvK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "# Enviroment variable\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-3.4.4-bin-hadoop3\"\n",
        "os.environ['PYSPARK_SUBMIT_ARGS'] = (\n",
        "    '--jars /content/mongo-spark-connector_2.12-10.4.1.jar,'\n",
        "    '/content/mongodb-driver-sync-4.10.2.jar,'\n",
        "    '/content/bson-4.10.2.jar,'\n",
        "    '/content/mongodb-driver-core-4.10.2.jar pyspark-shell'\n",
        ")"
      ],
      "metadata": {
        "id": "hDfr00GVoohp"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import findspark\n",
        "findspark.init()"
      ],
      "metadata": {
        "id": "Hs1JQTKbou1F"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Libraries for SQL Spark\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql import functions\n",
        "from pyspark.sql.functions import split, explode, trim, count, sum, col, current_date, lower, regexp_replace\n",
        "import time\n",
        "\n",
        "# Spark Session configuration\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"MongoDBAtlasConnection\") \\\n",
        "    .config(\"spark.mongodb.read.connection.uri\", mongo_uri) \\\n",
        "    .config(\"spark.mongodb.write.connection.uri\", mongo_uri) \\\n",
        "    .config(\"spark.jars\", \"/content/mongo-spark-connector_2.12-10.4.1.jar\") \\\n",
        "    .getOrCreate()\n",
        "print(spark)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l9l7YAupqE3N",
        "outputId": "99340b27-290f-4be5-ae35-6faca3ddd19c"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<pyspark.sql.session.SparkSession object at 0x7d2b798d1fd0>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Check if the connector works\n",
        "print(spark.sparkContext.getConf().get(\"spark.jars\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bGwmBC0Pz-a4",
        "outputId": "8339af25-07b7-47cc-fa5b-6367bef60466"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/mongo-spark-connector_2.12-10.4.1.jar\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Trying to Connect to mongoAtlas\n",
        "try:\n",
        "    df = spark.read \\\n",
        "        .format(\"mongodb\") \\\n",
        "        .option(\"database\", \"CAMPANIA_SALUTE\") \\\n",
        "        .option(\"collection\", \"ANAGRAFICA\") \\\n",
        "        .load()\n",
        "    print(\"Connessione riuscita! Ecco i primi 5 documenti:\")\n",
        "    df.show(5)\n",
        "except Exception as e:\n",
        "    print(\"Errore di connessione:\", str(e))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YKSuYxh76l69",
        "outputId": "cf7530ec-0628-4e2c-c3e4-6307e3fc7e6c"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Connessione riuscita! Ecco i primi 5 documenti:\n",
            "++\n",
            "||\n",
            "++\n",
            "++\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The number of tables (collections in MongoDB) we need to load is significant. To ensure an efficient workflow, we need to implement a proper organization system for this process."
      ],
      "metadata": {
        "id": "iP6mh1puy8hH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dictionary of all csv paths\n",
        "csvPaths = {}\n",
        "healthDB_path = os.path.join(dataset_path, '2024-05-05-DATABASE')\n",
        "for collection in collection_string_list:\n",
        "  csvPaths[collection] = os.path.join(healthDB_path,collection + '.csv')"
      ],
      "metadata": {
        "id": "V4ViEaa5suat"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(csvPaths['ANAGRAFICA'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cYi_pxTUxyHf",
        "outputId": "0c4b4944-dc21-429f-d628-478fc1f891f6"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/Colab Notebooks/Big Data/Final_Project/Dataset/2024-05-05-DATABASE/ANAGRAFICA.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import csv\n",
        "\n",
        "class CSVLoaderManager:\n",
        "    def __init__(self, spark: SparkSession, mongo_uri: str = None):\n",
        "        \"\"\"\n",
        "        The constructor initializes the CSVLoaderManager with a SparkSession and an optional MongoDB URI.\n",
        "        \"\"\"\n",
        "        self.spark = spark\n",
        "        self.mongo_uri = mongo_uri\n",
        "        self.datasets = {}\n",
        "\n",
        "    def _detect_delimiter(self, name, file_path: str, sample_size: int = 2048) -> str:\n",
        "        \"\"\"\n",
        "        The function reads a file part to infer the delimiter (CSV or TSV)\n",
        "        \"\"\"\n",
        "        with open(file_path, 'r', encoding='utf-8') as f:\n",
        "            # Reads some samples\n",
        "            sample = f.read(sample_size)\n",
        "            sniffer = csv.Sniffer()\n",
        "            try:\n",
        "                # Use sniffer for checking the delimiter\n",
        "                dialect = sniffer.sniff(sample)\n",
        "                print(f\"Name: {name}; Delimiter: {dialect.delimiter}\")\n",
        "                return dialect.delimiter\n",
        "            except csv.Error:\n",
        "                raise ValueError(f\"Unable to automatically detect the file delimiter format of: {file_path}\")\n",
        "\n",
        "\n",
        "    def load_csv(self, name: str, file_path: str) -> None:\n",
        "        \"\"\"\n",
        "        This function loads a CSV file into a Spark DataFrame,\n",
        "        it adds the dataframe to the manager's datasets\n",
        "        \"\"\"\n",
        "        delimiter = self._detect_delimiter(name, file_path)\n",
        "        ds = self.spark.read \\\n",
        "          .option(\"delimiter\",delimiter) \\\n",
        "          .option(\"inferSchema\", \"true\") \\\n",
        "          .option(\"header\", \"true\") \\\n",
        "          .option(\"multiline\", \"true\") \\\n",
        "          .option(\"quote\", \"\\\"\") \\\n",
        "          .option(\"escape\", \"\\\"\") \\\n",
        "          .csv(file_path)\n",
        "        self.datasets[name] = ds\n",
        "\n",
        "    def load_many(self, files) -> None:\n",
        "        \"\"\"\n",
        "        This function loads multiple CSV files into Spark DataFrames\n",
        "        \"\"\"\n",
        "        for name, path in files.items():\n",
        "            self.load_csv(name, path)\n",
        "\n",
        "    def get(self, name: str):\n",
        "        return self.datasets.get(name)\n",
        "\n",
        "    def set(self, name: str, df, overwrite: bool=True):\n",
        "      \"\"\"\n",
        "      This function sets a DataFrame in the manager's datasets,\n",
        "      it can overwrites the DataFrame if it already exists with the parameter 'overwrite'\n",
        "      \"\"\"\n",
        "      if not hasattr(df, 'schema'):\n",
        "        raise TypeError(\"The given value is not a Spark dataframe.\")\n",
        "      if name in self.datasets and not overwrite:\n",
        "        raise ValueError(f\"The dataset '{name}' already exists. Use overwrite=True.\")\n",
        "      self.datasets[name] = df\n",
        "\n",
        "    def list_datasets(self):\n",
        "      \"\"\"\n",
        "      This function returns a list of the names of the datasets in the manager\n",
        "      \"\"\"\n",
        "      return list(self.datasets.keys())\n",
        "\n",
        "    def save_to_mongo(self, name: str, database: str, mode: str = \"ignore\") -> None:\n",
        "      \"\"\"\n",
        "      This function saves a DataFrame to MongoDB ATLAS\n",
        "      \"\"\"\n",
        "      if name not in self.datasets:\n",
        "          raise ValueError(f\"Dataset '{name}' not found.\")\n",
        "      if not self.mongo_uri:\n",
        "          raise ValueError(\"Mongo URI not configured.\")\n",
        "      self.datasets[name].write \\\n",
        "          .format(\"mongodb\") \\\n",
        "          .mode(mode) \\\n",
        "          .option(\"database\", database) \\\n",
        "          .option(\"collection\", name) \\\n",
        "          .save()\n",
        "\n",
        "    def drop_collections(self, database: str, collections: list) -> None:\n",
        "      \"\"\"\n",
        "      Drops specified collections from the given MongoDB database.\n",
        "      \"\"\"\n",
        "      if not self.mongo_uri:\n",
        "          raise ValueError(\"Mongo URI not configured.\")\n",
        "      client = MongoClient(self.mongo_uri)\n",
        "      db = client[database]\n",
        "      for collection in collections:\n",
        "        if collection in db.list_collection_names():\n",
        "          db.drop_collection(collection)\n",
        "          print(f\"Collection '{collection}' dropped successfully.\")\n",
        "        else:\n",
        "          print(f\"Collection '{collection}' not found in the database.\")\n",
        "      client.close()\n"
      ],
      "metadata": {
        "id": "qm8OpdEPV9Be"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we can load all the collections into dataframe to manipulate them."
      ],
      "metadata": {
        "id": "6udv8wP83g8a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "manager = CSVLoaderManager(spark, mongo_uri)\n",
        "manager.load_many(csvPaths)\n",
        "manager.list_datasets()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-90TGTFAYCpI",
        "outputId": "e22c4554-6829-46a8-96c9-1d92aef7b497"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Name: ANAGRAFICA; Delimiter: ,\n",
            "Name: ANAMNESI; Delimiter: \t\n",
            "Name: CORONAROGRAFIA_PTCA; Delimiter: \t\n",
            "Name: ECOCARDIO_DATI; Delimiter: \t\n",
            "Name: ECOCAROTIDI; Delimiter: \t\n",
            "Name: ESAMI_LABORATORIO; Delimiter: \t\n",
            "Name: ESAMI_SPECIALISTICI; Delimiter: \t\n",
            "Name: ESAMI_STRUMENTALI_CARDIO; Delimiter: \t\n",
            "Name: LISTA_EVENTI; Delimiter: \t\n",
            "Name: PREVALENT; Delimiter: \t\n",
            "Name: RICOVERO_OSPEDALIERO; Delimiter: \t\n",
            "Name: VISITA_CONTROLLO_ECG; Delimiter: \t\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['ANAGRAFICA',\n",
              " 'ANAMNESI',\n",
              " 'CORONAROGRAFIA_PTCA',\n",
              " 'ECOCARDIO_DATI',\n",
              " 'ECOCAROTIDI',\n",
              " 'ESAMI_LABORATORIO',\n",
              " 'ESAMI_SPECIALISTICI',\n",
              " 'ESAMI_STRUMENTALI_CARDIO',\n",
              " 'LISTA_EVENTI',\n",
              " 'PREVALENT',\n",
              " 'RICOVERO_OSPEDALIERO',\n",
              " 'VISITA_CONTROLLO_ECG']"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "manager.get('LISTA_EVENTI').printSchema()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MUAeHzgvYyg3",
        "outputId": "1487835f-9f29-400d-8ebb-12dee2d27c59"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- SEZIONE: integer (nullable = true)\n",
            " |-- CODPAZ: integer (nullable = true)\n",
            " |-- DATA: date (nullable = true)\n",
            " |-- NUM_PROGRESSIVO: integer (nullable = true)\n",
            " |-- TIPO_EVENTO: string (nullable = true)\n",
            " |-- NUM_PROGRESSIVO_GLOBALE: integer (nullable = true)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ------ COSE DA FARE ----------\n",
        "\n",
        "### CONTINUA DA QUA, FAI LISTA COLONNE DA FARE LOWER CASE\n",
        "### FORMATTARE TUTTO A NONE\n",
        "### CONCLUSIONI IN ESAMI_STRUMENTALI_CARDIO UNKNOW-> NONE\n",
        "### CARICARE SU MONGO DB TUTTE LE COLLECTION\n",
        "### PROVARE A GENERARE DELLE QUERY CON LLM\n",
        "### INIZIARE MODULO GRAFICO PER STREAMLIT\n"
      ],
      "metadata": {
        "id": "ln3CKe0abyLX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In general all datasets are quite cleaned, but there is something that could give trouble in the future. For example some columns have same values but sometimes in lower case and other time in Upper case."
      ],
      "metadata": {
        "id": "hzIiz_Zr4eFE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's capitalize the columns that may give problems in query generation\n",
        "from pyspark.sql.functions import col, upper\n",
        "\n",
        "df = manager.get('ANAGRAFICA')\n",
        "\n",
        "upper_df = df \\\n",
        "      .withColumn(\"COGNOME\", upper(col(\"COGNOME\"))) \\\n",
        "      .withColumn(\"NOMEPAZ\", upper(col(\"NOMEPAZ\"))) \\\n",
        "      .withColumn(\"COMUNE_DI_NASCITA\", upper(col(\"COMUNE_DI_NASCITA\")))\n",
        "\n",
        "manager.set('ANAGRAFICA', upper_df)\n",
        "manager.get('ANAGRAFICA').show(10)"
      ],
      "metadata": {
        "id": "0hBRrmbbsbR7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e92bc465-3fd3-454e-8d78-f7192b764ee7"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+------+------------+----------+-------------+-----+-----------------+------------------------+----------------+----------------+--------------------+------------+\n",
            "|SEZIONE|CODPAZ|     COGNOME|   NOMEPAZ|DATADINASCITA|SESSO|COMUNE_DI_NASCITA|CODICE_COMUNE_DI_NASCITA|  CODICE_FISCALE|GATE_DI_INGRESSO|      MOTIVO_DECESSO|DATA_DECESSO|\n",
            "+-------+------+------------+----------+-------------+-----+-----------------+------------------------+----------------+----------------+--------------------+------------+\n",
            "|      1|     1|           A|     NELLO|   1937-02-16|    F|                -|                    null|---NLL37B56-----|         Esterno|                null|        null|\n",
            "|      1|     7|       NAVAS| MADDALENA|   1937-01-18|    F|           ACERRA|                    A024|NVSMDL37A58A024E|    Ipertensione|Causa extracardio...|  2019-07-02|\n",
            "|      1|    17|D`ALESSANDRO|  RAFFAELE|   1932-10-24|    M|                 |                        |DLSRFL32R24-----|    Ipertensione|                    |        null|\n",
            "|      1|    18|      GALANO|   GENNARO|   1956-04-21|    M|           NAPOLI|                    F839|GLNGNR56D21F839J|    Ipertensione|                null|        null|\n",
            "|      1|    20|      POLITO|   LORENZO|   1959-05-16|    M|          CARACAS|                        |PLTLNZ59E16-----|    Ipertensione|                    |        null|\n",
            "|      1|    24|     ORLANDO|     MARIO|   1940-04-03|    M|                 |                        |RLNMRA40D03-----|    Ipertensione|                null|        null|\n",
            "|      1|    25|    GIARDINO|COSTANTINO|   1932-01-23|    M|           NAPOLI|                    F839|GRDCTN32A23F839F|    Ipertensione|                    |        null|\n",
            "|      1|    27|   DI NATALE|  GIUSEPPE|   1943-07-16|    M|           NAPOLI|                    F839|DNTGPP43L16F839I|    Ipertensione|                null|        null|\n",
            "|      1|    28|    DE CONTE|  GIOVANNI|   1924-09-09|    M| NOCERA INFERIORE|                    F912|DCNGNN24P09F912D|    Ipertensione|                null|        null|\n",
            "|      1|    29|        ORTO|   ANTONIO|   1955-12-11|    M|           NAPOLI|                    F839|RTONTN55T11F839D|    Ipertensione|                    |        null|\n",
            "+-------+------+------------+----------+-------------+-----+-----------------+------------------------+----------------+----------------+--------------------+------------+\n",
            "only showing top 10 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Another problem is the presence of more \"null\" values in some columns."
      ],
      "metadata": {
        "id": "wQ9HAuqj6Pa_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def normalize_null(value):\n",
        "    \"\"\"\n",
        "    The function uniforms null or unexisting values at None.\n",
        "    \"\"\"\n",
        "    null_equivalents = {\"\", \" \", \"  \", \"null\", \"None\", \"N/A\", \"na\", \"-\", \"--\", \"NaN\"}\n",
        "\n",
        "    if isinstance(value, str):\n",
        "        value = value.strip()\n",
        "    return None if value in null_equivalents else value\n"
      ],
      "metadata": {
        "id": "lqk_ILt1hLpp"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import udf, col\n",
        "from pyspark.sql.types import StringType\n",
        "\n",
        "for name in collection_string_list:\n",
        "  df = manager.get(name)\n",
        "\n",
        "  norm_udf = udf(lambda x: normalize_null(x), StringType())\n",
        "  for column_name in df.columns:\n",
        "    df = df.withColumn(column_name, norm_udf(col(column_name)))\n",
        "\n",
        "  manager.set(name, df)\n"
      ],
      "metadata": {
        "id": "HnfV9tz_hW4k"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Drop Collection 🚩***Run it only if you want to eliminate some collection from mongo atlas***"
      ],
      "metadata": {
        "id": "OPNBx4i9FwcV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "manager.drop_collections(\"CAMPANIA_SALUTE\", [\"LISTA_EVENTI\"])"
      ],
      "metadata": {
        "id": "Si_hdxX_F2HB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Loading in Mongo DB"
      ],
      "metadata": {
        "id": "QBml5lFKlcRg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We'll now load our processed datasets into MongoDB Atlas using PySpark's native connector. This efficient approach enables seamless integration between Spark DataFrames and MongoDB collections. The following configuration ensures optimal performance and reliability."
      ],
      "metadata": {
        "id": "MinY9EEhWUtj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pymongo"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K8JCIBM2A5pK",
        "outputId": "4edf78e5-772a-4444-c203-761b31bc8491"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pymongo\n",
            "  Downloading pymongo-4.13.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (22 kB)\n",
            "Collecting dnspython<3.0.0,>=1.16.0 (from pymongo)\n",
            "  Downloading dnspython-2.7.0-py3-none-any.whl.metadata (5.8 kB)\n",
            "Downloading pymongo-4.13.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.4 MB)\n",
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/1.4 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m40.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dnspython-2.7.0-py3-none-any.whl (313 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m313.6/313.6 kB\u001b[0m \u001b[31m22.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: dnspython, pymongo\n",
            "Successfully installed dnspython-2.7.0 pymongo-4.13.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for name in collection_string_list:\n",
        "  manager.save_to_mongo(name, \"CAMPANIA_SALUTE\", mode=\"overwrite\")"
      ],
      "metadata": {
        "id": "J_quaNgmm9vI",
        "collapsed": true
      },
      "execution_count": 20,
      "outputs": []
    }
  ]
}